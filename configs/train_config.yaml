# ============================================================================
# CHAMPIONSHIP SR TRAINING - CHAMPIONSHIP ARCHITECTURE CONFIGURATION
# ============================================================================
# Hardware: Single GPU (26GB VRAM)
# Experts: HAT-L, DRCT-L, GRL-B, EDSR-L (4-expert ensemble)
# Target: 35.5 dB PSNR
# Parameters: ~1.2M trainable, ~131.7M frozen
# ============================================================================

experiment_name: "championship_sr_phase3_single_gpu"
description: "Phase 3 - 4-expert fusion (HAT+DRCT+GRL+EDSR) - Single GPU 26GB - 200 epochs"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  type: "CompleteEnhancedFusionSR"
  scale: 4
  
  experts:
    - name: "HAT"
      weight_path: "pretrained/hat/HAT-L_SRx4_ImageNet-pretrain.pth"
      frozen: true
      architecture:
        type: "HAT-L"
        embed_dim: 180
        depths: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
        num_heads: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
        window_size: 16
        
    - name: "DRCT"
      weight_path: "pretrained/drct/DRCT-L_X4.pth"
      frozen: true
      architecture:
        type: "DRCT-L"
        embed_dim: 180
        depths: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
        num_heads: [6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]
        window_size: 16
        
    - name: "GRL"
      weight_path: "pretrained/grl/GRL-B_SR_x4.pth"
      frozen: true
      architecture:
        type: "GRL-B"
        embed_dim: 180
        depths: [4, 4, 8, 8, 4, 4]
        num_heads: [6, 6, 6, 6, 6, 6]
        window_size: 8
        
    - name: "EDSR"
      weight_path: "pretrained/edsr/EDSR_Lx4_f256b32_DIV2K_official-76ee1c8f.pth"
      frozen: true
      architecture:
        type: "EDSR-L"
        num_feat: 256
        num_block: 32
  
  fusion:
    num_experts: 4
    fusion_dim: 128             # Collaborative learning dimension
    refine_channels: 128        # Refinement network channels
    refine_depth: 6             # Refinement network depth
    base_channels: 64           # Hierarchical fusion base channels
    use_residual: true
    use_multiscale: true
    use_quality_head: true
    
    # Championship improvements (ALL ENABLED)
    improvements:
      dynamic_expert_selection: true    # Phase 6: per-pixel difficulty gating
      cross_band_attention: true        # Phase 3: LKA k=21 cross-band (9 bands)
      adaptive_frequency_bands: true    # Phase 2: DCT+DWT+FFT decomposition
      multi_resolution_fusion: true     # Phase 5: hierarchical 64→128→256
      collaborative_learning: true      # Phase 4: LKA cross-expert attention
      edge_enhancement: true            # Phase 7b: Laplacian pyramid refinement

# ============================================================================
# TSD-SR CONFIGURATION (Phase 7 - Perceptual Refinement)
# ============================================================================
tsdsr:
  enabled: true
  use_during_training: false
  student_path: "pretrained/tsdsr/transformer.safetensors"
  teacher_path: "pretrained/teacher/teacher.safetensors"
  vae_path: "pretrained/tsdsr/vae.safetensors"
  inference_steps: 1
  use_teacher_for_validation: false

# ============================================================================
# TRAINING CONFIGURATION (SINGLE GPU - 26GB)
# ============================================================================
training:
  total_epochs: 200
  
  # Single GPU settings (26GB VRAM)
  batch_size: 32
  num_workers: 4
  prefetch_factor: 2
  persistent_workers: true
  pin_memory: true
  
  # Precision and optimization
  precision: "fp16"
  use_amp: true
  gradient_clip: 1.0
  accumulation_steps: 1
  
  # Optimizer
  optimizer:
    type: "AdamW"
    lr: 1.0e-4
    betas: [0.9, 0.999]
    weight_decay: 1.0e-4
    eps: 1.0e-8
  
  # Learning rate scheduler
  scheduler:
    type: "CosineAnnealingWarmRestarts"
    T_0: 50
    T_mult: 2
    eta_min: 5.0e-8
    warmup_epochs: 5
    warmup_lr: 5.0e-7
  
  # EMA for stable inference
  ema:
    enabled: true
    decay: 0.9995

# ============================================================================
# LOSS CONFIGURATION (3-STAGE)
# ============================================================================
loss:
  # Stage-based loss weights (championship strategy)
  stages:
    # Stage 1: Pure L1 reconstruction (0-80 epochs)
    - epochs: [0, 80]
      stage_name: "foundation_psnr"
      description: "Build strong pixel-level reconstruction"
      weights:
        l1: 1.0
        charbonnier: 0.0
        swt: 0.0
        fft: 0.0
        ssim: 0.0
        vgg: 0.0
    
    # Stage 2: Add frequency losses (80-150 epochs)
    - epochs: [80, 150]
      stage_name: "frequency_refinement"
      description: "Enhance frequency detail with SWT + FFT"
      weights:
        l1: 0.75
        charbonnier: 0.0
        swt: 0.20
        fft: 0.05
        ssim: 0.0
        vgg: 0.0
    
    # Stage 3: Detail enhancement (150-200 epochs)
    - epochs: [150, 200]
      stage_name: "detail_enhancement"
      description: "Final edge and texture refinement"
      weights:
        l1: 0.60
        charbonnier: 0.0
        swt: 0.25
        fft: 0.10
        ssim: 0.05
        vgg: 0.0
  
  # Loss components configuration
  l1:
    enabled: true
  
  charbonnier:
    enabled: false
    eps: 1.0e-6
  
  swt:
    enabled: true
    levels: 3
    wavelet: "db4"
    use_gpu_approximation: false
  
  fft:
    enabled: true
    loss_type: "l1"
    log_scale: true
  
  ssim:
    enabled: true
    window_size: 11
    use_padding: true
  
  vgg:
    enabled: false
    layers: ["relu2_2", "relu3_4", "relu4_4"]
    weights: [0.5, 0.3, 0.2]
  
  edge:
    enabled: false
  
  clip:
    enabled: false

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
dataset:
  train:
    root: "dataset/DF2K"
    hr_subdir: "train_HR"
    lr_subdir: "train_LR"
    cache_data: false
  
  val:
    root: "dataset/DF2K"
    hr_subdir: "val_HR"
    lr_subdir: "val_LR"
  
  lr_patch_size: 64
  scale: 4
  
  augmentation:
    enabled: true
    use_flip: true
    flip_prob: 0.5
    use_rotation: true
    rotation_prob: 0.5
    rotation_angles: [90, 180, 270]
    use_color_jitter: true
    color_jitter_prob: 0.2
    brightness: 0.05
    contrast: 0.05
    saturation: 0.05
    use_cutblur: false
    use_mixup: false
    use_random_crop_scale: true
    crop_scale_range: [0.9, 1.1]
  
  repeat_factor: 20

# ============================================================================
# VALIDATION CONFIGURATION
# ============================================================================
validation:
  validate_every: 5
  validate_start: 10
  metrics:
    - "psnr"
    - "ssim"
    - "lpips"
  crop_border: 4
  test_y_channel: true
  log_images: true
  log_images_every: 10
  num_log_images: 8
  save_best_images: true
  save_comparison_grid: true

# ============================================================================
# CHECKPOINT CONFIGURATION
# ============================================================================
checkpoint:
  checkpoint_dir: "checkpoints/phase3_single_gpu"
  save_every: 10
  save_lightweight: true
  keep_best_k: 5
  keep_last_n: 10
  keep_milestone: [50, 100, 150, 200]
  metric: "psnr"
  mode: "max"
  resume: null
  load_optimizer: true
  load_scheduler: true
  load_ema: true

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  tensorboard:
    enabled: true
    log_dir: "logs/phase3_single_gpu"
    log_scalars_every: 50
    log_images_every: 500
  
  wandb:
    enabled: false
    project: "championship_sr"
    entity: null
    name: "phase3_single_gpu"
  
  print_freq: 50
  log_loss_components: true
  log_gradients: false
  log_learning_rate: true
  log_ema_metrics: true
  log_phase_info: true
  estimate_remaining_time: true
  log_gpu_memory: true
  log_training_speed: true

# ============================================================================
# HARDWARE (SINGLE GPU)
# ============================================================================
hardware:
  gpu_ids: [0]
  cudnn_benchmark: true
  cudnn_deterministic: false
  empty_cache_every: 100
  max_memory_allocated: null
  use_ddp: false
  find_unused_parameters: false

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
seed: 42
deterministic: false

# ============================================================================
# MONITORING
# ============================================================================
monitoring:
  track_metrics:
    - name: "train_loss"
      mode: "min"
    - name: "val_psnr"
      mode: "max"
    - name: "val_ssim"
      mode: "max"
  
  early_stopping:
    enabled: false
    patience: 30
    min_delta: 0.01
    metric: "val_psnr"
  
  reduce_lr_on_plateau:
    enabled: false
    patience: 10
    factor: 0.5
    min_lr: 1.0e-7