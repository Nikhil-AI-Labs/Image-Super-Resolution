# ============================================================================
# NTIRE 2025 SR Training Configuration
# ============================================================================
# Championship Training for Frequency Mixer (PSNR Optimization)
# ============================================================================

# Experiment name
experiment_name: "frequency_mixer_psnr_v1"
description: "Training frequency mixer with frozen experts for PSNR optimization"

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================
model:
  type: "MultiFusionSR"
  scale: 4
  
  # Expert models (frozen, pre-trained)
  experts:
    - name: "HAT"
      weight_path: "pretrained/HAT_SRx4.pth"
      frozen: true
    - name: "MambaIR"
      weight_path: "pretrained/MambaIR_SR_x4.pth"
      frozen: true
    - name: "NAFNet"
      weight_path: "pretrained/NAFNet_x4.pth"
      frozen: true
  
  # Fusion network configuration
  fusion:
    num_experts: 3
    use_residual: true
    use_multiscale: true
    use_quality_head: true

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  # Epochs and batch size
  total_epochs: 200
  batch_size: 8  # Reduced for FP32 + large experts (adjust based on GPU)
  num_workers: 4
  pin_memory: true
  
  # Precision and optimization
  precision: "fp32"  # No mixed precision (HAT attention stability)
  use_amp: false
  gradient_clip: 1.0  # Gradient clipping for stability
  accumulation_steps: 2  # Effective batch = 8 * 2 = 16
  
  # Learning rate
  optimizer:
    type: "AdamW"
    lr: 2.0e-4  # Higher LR with warmup
    betas: [0.9, 0.999]
    weight_decay: 1.0e-4
    eps: 1.0e-8
  
  # Learning rate scheduler
  scheduler:
    type: "CosineAnnealingWarmRestarts"
    T_0: 50  # First restart at 50 epochs
    T_mult: 2  # Double period after each restart
    eta_min: 1.0e-7
    warmup_epochs: 5  # Linear warmup for first 5 epochs
    warmup_lr: 1.0e-6
  
  # EMA for stable inference
  ema:
    enabled: true
    decay: 0.999

# ============================================================================
# LOSS CONFIGURATION (MULTI-STAGE)
# ============================================================================
loss:
  # Stage-based loss weights (championship strategy)
  stages:
    # Stage 1: PSNR-focused initialization (0-100 epochs)
    - epochs: [0, 100]
      stage_name: "reconstruction"
      weights:
        l1: 1.0
        charbonnier: 0.0
        swt: 0.0
        vgg: 0.0
        fft: 0.0
        ssim: 0.0
    
    # Stage 2: Add frequency loss (100-150 epochs)
    - epochs: [100, 150]
      stage_name: "frequency_refinement"
      weights:
        l1: 0.8
        charbonnier: 0.0
        swt: 0.15
        vgg: 0.0
        fft: 0.05
        ssim: 0.0
    
    # Stage 3: Detail enhancement (150-200 epochs)
    - epochs: [150, 200]
      stage_name: "detail_enhancement"
      weights:
        l1: 0.6
        charbonnier: 0.0
        swt: 0.25
        vgg: 0.0
        fft: 0.1
        ssim: 0.05
  
  # Loss components configuration
  l1:
    enabled: true
  
  charbonnier:
    enabled: false
    eps: 1.0e-6
  
  swt:
    enabled: true
    levels: 3
    wavelet: "haar"
    use_gpu_approximation: false  # Accurate for training
  
  fft:
    enabled: true
    loss_type: "l1"
  
  ssim:
    enabled: true
    window_size: 11
  
  vgg:
    enabled: false  # Optional for perceptual quality
    layers: ["relu2_2", "relu3_4", "relu4_4"]
  
  edge:
    enabled: false  # Optional for edge preservation
  
  clip:
    enabled: false  # Save for final refinement

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
dataset:
  # Training dataset paths
  train:
    root: "data/DF2K"
    hr_subdir: "train_HR"
    lr_subdir: "train_LR"
  
  # Validation dataset paths
  val:
    root: "data/DF2K"
    hr_subdir: "val_HR"
    lr_subdir: "val_LR"
  
  # Patch settings
  lr_patch_size: 64  # LR patch size (HR will be 256 for scale 4)
  scale: 4
  
  # Augmentation
  augmentation:
    enabled: true
    use_flip: true
    use_rotation: true
    use_color_jitter: true
    use_cutblur: false
    flip_prob: 0.5
    rotation_prob: 0.5
    color_jitter_prob: 0.3
  
  # Dataset repetition (more diverse patches per epoch)
  repeat_factor: 20

# ============================================================================
# VALIDATION CONFIGURATION
# ============================================================================
validation:
  # Validation frequency
  validate_every: 5  # Validate every N epochs
  
  # Metrics to compute
  metrics:
    - "psnr"
    - "ssim"
  
  # Metric calculation settings (same as official NTIRE evaluation)
  crop_border: 4  # Crop 4 pixels for scale 4
  test_y_channel: true  # Calculate on Y channel
  
  # Image logging
  log_images: true
  log_images_every: 10
  num_log_images: 4

# ============================================================================
# CHECKPOINT CONFIGURATION
# ============================================================================
checkpoint:
  # Checkpoint directory
  checkpoint_dir: "checkpoints"
  
  # Checkpoint saving
  save_every: 10  # Save every N epochs
  keep_best_k: 3  # Keep top 3 best checkpoints
  keep_last_n: 5  # Keep last 5 regular checkpoints
  
  # Best checkpoint criteria
  metric: "psnr"
  mode: "max"  # Higher PSNR is better
  
  # Resume training
  resume: null  # Path to checkpoint (null = start fresh)
  load_optimizer: true

# ============================================================================
# LOGGING CONFIGURATION
# ============================================================================
logging:
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "logs"
  
  # Console logging
  print_freq: 50  # Print every N iterations
  
  # What to log
  log_loss_components: true
  log_gradients: false
  log_learning_rate: true
  log_ema_metrics: true

# ============================================================================
# HARDWARE CONFIGURATION
# ============================================================================
hardware:
  # GPU settings
  gpu_ids: [0]
  cudnn_benchmark: true
  
  # Memory management
  empty_cache_every: 100  # Clear CUDA cache periodically

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
seed: 42
deterministic: false  # Set true for full reproducibility (slower)
